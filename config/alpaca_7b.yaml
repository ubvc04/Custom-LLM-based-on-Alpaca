# 🏆 Alpaca Domination 7B Configuration
# Targeting #1 global performance among Alpaca-trained models

model:
  name: "alpaca-domination-7b"
  size: "7b"
  base_model: "meta-llama/Llama-2-7b-hf"
  
  # Architecture parameters
  vocab_size: 32000
  hidden_size: 4096
  intermediate_size: 11008
  num_hidden_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 32
  max_position_embeddings: 32768
  
  # Advanced features
  use_flash_attention: true
  use_gradient_checkpointing: true
  use_constitutional_loss: true
  constitutional_weight: 0.1
  
  # Efficiency features
  use_mixed_precision: true
  use_moe: false

training:
  # Data configuration
  data_dir: "data"
  max_seq_length: 2048
  
  # Batch configuration
  train_batch_size: 4
  eval_batch_size: 8
  gradient_accumulation_steps: 32
  effective_batch_size: 128  # train_batch_size * gradient_accumulation_steps
  
  # Training parameters
  num_epochs: 3
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  max_grad_norm: 1.0
  
  # Optimization
  optimizer: "adamw"
  lr_scheduler: "cosine_with_warmup"
  use_8bit_adam: true
  
  # Precision
  fp16: false
  bf16: true
  
  # Advanced training features
  use_deepspeed: true
  use_gradient_checkpointing: true
  group_by_length: true
  
  # LoRA/QLoRA (optional)
  use_lora: false
  use_qlora: false
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_target_modules:
    - "q_proj"
    - "k_proj" 
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

evaluation:
  eval_strategy: "steps"
  eval_steps: 500
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  load_best_model_at_end: true
  
  # Benchmarks to run
  benchmarks:
    - "mmlu"
    - "hellaswag" 
    - "arc_challenge"
    - "truthfulqa"
    - "gsm8k"
    - "humaneval"

monitoring:
  # Logging
  logging_steps: 50
  logging_dir: "logs"
  
  # Checkpointing
  save_steps: 1000
  save_total_limit: 3
  output_dir: "experiments/alpaca-domination-7b"
  
  # Weights & Biases
  use_wandb: true
  wandb_project: "alpaca-domination"
  wandb_entity: null
  
  # TensorBoard
  use_tensorboard: true

hardware:
  # GPU configuration
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  
  # Memory optimization
  use_cpu_offload: false
  max_memory_mb: 24000  # For RTX 4090
  
  # Distributed training
  use_distributed: true
  world_size: 1

targets:
  # Performance benchmarks (NON-NEGOTIABLE)
  mmlu_score: 0.70          # Target: >70%
  hellaswag_score: 0.85     # Target: >85%
  arc_challenge_score: 0.65 # Target: >65%
  truthfulqa_score: 0.55    # Target: >55%
  gsm8k_score: 0.60         # Target: >60%
  humaneval_score: 0.50     # Target: >50%
  
  # Technical performance
  inference_speed_ms: 50    # <50ms first token
  tokens_per_second: 100    # >100 tokens/second
  memory_usage_gb: 8        # <8GB VRAM with quantization
  context_length: 32768     # 32K context support
  
  # Quality metrics
  conversation_quality: 0.90  # >90% win rate vs existing Alpaca models
  safety_score: 0.95          # >95% safety alignment
  uptime: 0.98                # >98% production uptime
